---
title: "In-class Exercise 10: Text Visual Analytics with R"
subtitle: ""  
author: "Dr. Kam Tin Seong<br/>Assoc. Professor of Information Systems"
institute: "School of Computing and Information Systems,<br/>Singapore Management University"
date: "2020-7-4 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [xaringan-themer.css, "css/text.css"]
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "js/macros.js"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina=3,
                      echo = TRUE,
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#1381B0",
  secondary_color = "#FF961C",
  inverse_header_color = "#FFFFFF"
)
```

## Overview

.large[
In this hands-on exercise, you will learn how to visualising network data using R.

By the end of this hands-on exercise, you will be able to:

- create graph object data frames, manipulate them using appropriate functions of *dplyr*, *lubridate*, and *tidygraph*,
- build network graph visualisation using appropriate functions of *ggraph*,
- compute network geometrics using *tidygraph*,
- build advanced graph visualisation by incorporating the network geometrics, and
- build interactive network visualisation using *visNetwork* package. 
]

---
## Getting Started
### Installing and launching R packages

.pull-left[
In this hands-on exercise, four network data modelling and visualisation packages will be installed and launched.  They are igraph, tidygraph, ggraph and visNetwork.  Beside these four packages, tidyverse and [lubridate](https://lubridate.tidyverse.org/), an R package specially designed to handle and wrangling time data will be installed and launched too.]

.pull-right[
The code chunk:

```{r, echo=TRUE, eval=TRUE}
packages = c('tidytext', 'igraph', 
             'tidygraph', 'ggraph', 
             'widyr', 'wordcloud',
             'DT', 'ggwordcloud', 
             'textplot', 'tidyverse')

for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}
```
]

---
## Introducing tidytext

.pull-left[
- Using tidy data principles in processing, analysing and visualising text data.
- Much of the infrastructure needed for text mining with tidy data frames already exists in packages like 'dplyr', 'broom', 'tidyr', and 'ggplot2'.]

---
### The Text Visual Analytics Workflow

Figure below shows the workflow using tidytext approach

.center[
![](img/image10-1.jpg)
]

---
### Importing Multiple Text Files from Multiple Folders

.pull-left[

#### Step 1: Creating a folder list

```{r}
news20 <- "data/20news/"
```

#### Step 2: Define a function to read all files from a folder into a data frame

```{r}
read_folder <- function(infolder) {
  tibble(file = dir(infolder, 
                    full.names = TRUE)) %>%
    mutate(text = map(file, 
                      read_lines)) %>%
    transmute(id = basename(file), 
              text) %>%
    unnest(text)
}
```
]

---
### Importing Multiple Text Files from Multiple Folders

.pull-left[
#### Step 3: Reading in all the messages from the 20news folder

```{r}
raw_text <- tibble(folder = 
                     dir(news20, 
                         full.names = TRUE)) %>%
  mutate(folder_out = map(folder, 
                          read_folder)) %>%
  unnest(cols = c(folder_out)) %>%
  transmute(newsgroup = basename(folder), 
            id, text)
```
]

.pull-right[
Things to learn from the code chunk above:
- [*read_lines()*](https://readr.tidyverse.org/reference/read_lines.html) of [**readr**](https://readr.tidyverse.org/index.html) package is used to read up to n_max lines from a file.
- [*map()*](https://purrr.tidyverse.org/reference/map.html) of [**purrr**](https://purrr.tidyverse.org/index.html) package is used to transform their input by applying a function to each element of a list and returning an object of the same length as the input.
- [*unnest()*](https://tidyr.tidyverse.org/reference/nest.html) of **dplyr** package is used to flatten a list-column of data frames back out into regular columns.
]

---
### Initial EDA 

.pull-left[
Figure below shows the frequency of messages by newsgroup.

```{r echo=FALSE}
raw_text %>%
  group_by(newsgroup) %>%
  summarize(messages = n_distinct(id)) %>%
  ggplot(aes(messages, newsgroup)) +
  geom_col(fill = "lightblue") +
  labs(y = NULL)
```
]

--
.pull-right[
The code chunk:

```{r eval=FALSE}
raw_text %>%
  group_by(newsgroup) %>%
  summarize(messages = n_distinct(id)) %>%
  ggplot(aes(messages, newsgroup)) +
  geom_col(fill = "lightblue") +
  labs(y = NULL)
```
]

---
### Cleaning Text Data

.pull-left[
#### Step 1: Removing header and automated email signitures

Notice that each message has some structure and extra text that we don’t want to include in our analysis. For example, every message has a header, containing field such as “from:” or “in_reply_to:” that describe the message. Some also have automated email signatures, which occur after a line like "--".

```{r}
cleaned_text <- raw_text %>%
  group_by(newsgroup, id) %>%
  filter(cumsum(text == "") > 0,
         cumsum(str_detect(
           text, "^--")) == 0) %>%
  ungroup()
```
]

.pull-right[
Things to learn from the code chunk:
- [*cumsum()*](https://rdrr.io/r/base/cumsum.html) of base R is used to return a vector whose elements are the cumulative sums of the elements of the argument.  
- [*str_detect()*](https://stringr.tidyverse.org/reference/str_detect.html) from **stringr** is used to detect the presence or absence of a pattern in a string.
]

---
### Cleaning Text Data

.pull-left[
#### Step 2: Removing lines with nested text representing quotes from other users. 

In this code chunk below, regular expressions are used to remove with nested text representing quotes from other users. 

```{r}
cleaned_text <- cleaned_text %>%
  filter(str_detect(text, "^[^>]+[A-Za-z\\d]")
         | text == "",
         !str_detect(text, 
                     "writes(:|\\.\\.\\.)$"),
         !str_detect(text, 
                     "^In article <")
  )
```
]

---
### Text Data Processing

.pull-left[
In this code chunk below, [*unnest_tokens()*](https://www.rdocumentation.org/packages/tidytext/versions/0.3.1/topics/unnest_tokens) of **tidytext** package is used to split the dataset into tokens, while [*stop_words()*](https://rdrr.io/cran/tidytext/man/stop_words.html) is used to remove stop-words. 


```{r}
usenet_words <- cleaned_text %>%
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$"),
         !word %in% stop_words$word)

```
]

---
### Visualising Words in newsgroups

Now that we’ve removed the headers, signatures, and formatting, we can start exploring common words. For starters, we could find the most common words in the entire dataset, or within particular newsgroups.

```{r}
usenet_words %>%
  count(word, sort = TRUE)
```


---

```{r}
words_by_newsgroup <- usenet_words %>%
  count(newsgroup, word, sort = TRUE) %>%
  ungroup()
```

---
### Visualising Words in newsgroups

.pull-left[
In this code chunk below, *wordcloud()* of **wordcloud** package is used to plot a static wordcloud.

```{r}
wordcloud(words_by_newsgroup$word,
          words_by_newsgroup$n,
          max.words = 300)

```
]

.pull-right[
A DT table can be used to complement the visual discovery.

```{r echo=FALSE}
DT::datatable(words_by_newsgroup, 
              filter = 'top') %>% 
  formatStyle(0, target = 'row', 
              lineHeight='25%')
```
]

---
### Visualising Words in newsgroups

.pull-left[
The wordcloud below is plotted by using [**ggwordcloud**](https://lepennec.github.io/ggwordcloud/) package.

```{r echo=FALSE}
set.seed(1234)

words_by_newsgroup %>%
  filter(n > 5) %>%
ggplot(aes(label = word,
           size = n)) +
  geom_text_wordcloud() +
  theme_minimal() +
  facet_wrap(~newsgroup)
```
]

--
.pull-left[
The code chunk used:

```{r eval=FALSE}
set.seed(1234)

words_by_newsgroup %>%
  filter(n > 0) %>%
ggplot(aes(label = word,
           size = n)) +
  geom_text_wordcloud() +
  theme_minimal() +
  facet_wrap(~newsgroup)
```
]



---
### Computing tf-idf within newsgroups

.pull-left[
The code chunk below uses *bind_tf_idf()* of tidytext to compute and bind the term frequency, inverse document frequency and ti-idf of a tidy text dataset to the dataset.   

```{r}
tf_idf <- words_by_newsgroup %>%
  bind_tf_idf(word, newsgroup, n) %>%
  arrange(desc(tf_idf))
```
]

---
### Visualising tf-idf as interactive table

Table below is an interactive table created by using *datatable()*.

```{r echo=FALSE}
DT::datatable(tf_idf, filter = 'top') %>% 
  formatRound(columns = c('tf', 'idf', 'tf_idf'), 
              digits = 3) %>%
  formatStyle(0, 
              target = 'row', 
              lineHeight='25%')
```

---
### Visualising tf-idf as interactive table

.pull-left[
The code chunk below uses *datatable()* of DT package to create a html table that allows pagination of rows and columns.

```{r eval=FALSE}
DT::datatable(tf_idf, filter = 'top') %>% 
  formatRound(columns = c('tf', 'idf', 
                          'tf_idf'), 
              digits = 3) %>%
  formatStyle(0, 
              target = 'row', 
              lineHeight='25%')
```
]

.pull-right[
Things to learn from the code chunk:
- *filter* argument is used to turn control the filter UI.
- *formatRound()* is used to customise the values format.  The argument *digits* define the number of decimal places.
- *formatStyle()* is used to customise the output table.  In this example, the arguments *target* and *lineHeight* are used to reduce the line height by 25%.

To learn more about customising DT's table, visit this [link](https://rstudio.github.io/DT/functions.html).
]

---
### Visualising tf-idf within newsgroups

.pull-left[
Facet bar charts technique is used to visualise the tf-idf values of science related newsgroup.

```{r echo=FALSE}
tf_idf %>%
  filter(str_detect(newsgroup, "^sci\\.")) %>%
  group_by(newsgroup) %>%
  slice_max(tf_idf, n = 12) %>%
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill = newsgroup)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ newsgroup, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```
]

--
.pull-right[
The code chunk used to prepare the plot.

```{r eval=FALSE}
tf_idf %>%
  filter(str_detect(newsgroup, "^sci\\.")) %>%
  group_by(newsgroup) %>%
  slice_max(tf_idf, 
            n = 12) %>%
  ungroup() %>%
  mutate(word = reorder(word, 
                        tf_idf)) %>%
  ggplot(aes(tf_idf, 
             word, 
             fill = newsgroup)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ newsgroup, 
             scales = "free") +
  labs(x = "tf-idf", 
       y = NULL)
```
]

---
### Counting and correlating pairs of words with the widyr package

- To count the number of times that two words appear within the same document, or to see how correlated they are. 
- Most operations for finding pairwise counts or correlations need to turn the data into a wide matrix first.
- [**widyr**](https://cran.r-project.org/web/packages/widyr/index.html) package first ‘casts’ a tidy dataset into a wide matrix, performs an operation such as a correlation on it, then re-tidies the result. 

.pull-left[
![](img/image10-2.jpg)
]

--
.pull-right[
In this code chunk below, *pairwise_cor()* of **widyr** package is used to compute the correlation between newsgroup based on the common words found. 

```{r}
newsgroup_cors <- words_by_newsgroup %>%
  pairwise_cor(newsgroup, 
               word, 
               n, 
               sort = TRUE)
```
]

---
### Visualising correlation as a network

.pull-left[
Now, we can visualise the relationship between newgroups in network graph as shown below. 

```{r echo=FALSE}
set.seed(2017)

newsgroup_cors %>%
  filter(correlation > .025) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(alpha = correlation, 
                     width = correlation)) +
  geom_node_point(size = 6, 
                  color = "lightblue") +
  geom_node_text(aes(label = name),
                 color = "red",
                 repel = TRUE) +
  theme_void()
```
]

--
.pull-right[
The code chunk:

```{r eval=FALSE}
set.seed(2017)

newsgroup_cors %>%
  filter(correlation > .025) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(alpha = correlation, 
                     width = correlation)) +
  geom_node_point(size = 6, 
                  color = "lightblue") +
  geom_node_text(aes(label = name),
                 color = "red",
                 repel = TRUE) +
  theme_void()
```
]

---
## Bigram


```{r}
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, 
                text, 
                token = "ngrams", 
                n = 2)
```

---
## References

#### widyr

- Reference guide
    - [widyr: Widen, process, and re-tidy a dataset](https://cran.r-project.org/web/packages/widyr/vignettes/intro.html)
    - [United Nations Voting Correlations](https://cran.r-project.org/web/packages/widyr/vignettes/united_nations.html)


